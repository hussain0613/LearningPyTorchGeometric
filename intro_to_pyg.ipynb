{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch Geometric by Examples\n",
    "Learning from: https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handling of Graphs and `torch_geometric.data.Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch_geometric.data.Data`\n",
    "A single graph in PyG is described by an instance of `torch_geometric.data.Data`, which holds the following attributes by default:\n",
    "\n",
    "- `data.x`: Node feature matrix with shape `[num_nodes, num_node_features]`, i.e., $n_f \\times d_f$ where $n_f$ is the number of nodes and $d_f$ is the number of features per node.\n",
    "- `data.edge_index`: Graph connectivity in [COO format](https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs) with shape `[2, num_edges]` and type `torch.long`\n",
    "    - COO(coordinate) format is a sparse matrix storage format.\n",
    "- `data.edge_attr`: Edge feature matrix with shape `[num_edges, num_edge_features]`, i.e., $n_e \\times d_e$ where $n_e$ is the number of edges and $d_e$ is the number of features per edge.\n",
    "- `data.y`: Target to train against (may have arbitrary shape), e.g., node-level targets of shape `[num_nodes, *]` or graph-level targets of shape `[1, *]` or edge-level targets of shape `[num_edges, *]`.\n",
    "- `data.pos`: Node position matrix with shape `[num_nodes, num_dimensions]`, e.g., spatial coordinates.\n",
    "\n",
    "Note: none of these attributes are required. They can be removed, replaced or extended dynamically via `data.yo = ...` assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 1], edge_index=[2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor([\n",
    "    [0, 1, 1, 2],\n",
    "    [1, 0, 2, 1],\n",
    "], dtype=torch.long)\n",
    "# this defines a graph with 3 nodes and 4 directed edges or 2 undirected edges\n",
    "# 0->1, 1->0, 1->2, 2->1 or 0<->1, 1<->2\n",
    "\n",
    "x = torch.tensor([\n",
    "    [-1], [0], [1]\n",
    "], dtype=torch.float) \n",
    "# this defines the node features, here we have 3 nodes and each node has 1 feature\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "# this defines the graph data structure\n",
    "# x: node features, edge_index: edge index tensor\n",
    "\n",
    "print(data) # data.__str__() = data.__repr__()\n",
    "data # data.__repr__() -> gives us the shape of the tensors, x and edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index2 = torch.tensor([\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 2],\n",
    "    [2, 1]\n",
    "])\n",
    "# this defines a graph with 3 nodes and 4 directed edges or 2 undirected edges\n",
    "# same as the previous `edge_index`, but in a different format\n",
    "\n",
    "data2 = Data(x=x, edge_index=edge_index2.t().contiguous())\n",
    "\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some of the `Data` attributes and methods\n",
    "Note: `Data` kind of works like a python `dict` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.validate(raise_on_error=False) # checks if the data object is valid\n",
    "# one of the checks it runs is to see if the edge_index values are within the range of the number of nodes\n",
    "# the values must be between 0 and the number of nodes - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseData.keys of Data(x=[3, 1], edge_index=[2, 4])>\n"
     ]
    }
   ],
   "source": [
    "print(data.keys) # something's wrong here! `data.keys` is a method, not an attribute, yet can't be called as a method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edge_index', 'x']\n"
     ]
    }
   ],
   "source": [
    "print(data.keys()) # this is how we call a method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.],\n",
      "        [ 0.],\n",
      "        [ 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(data['x']) # this is how we access the node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 2],\n",
       "        [1, 0, 2, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 2],\n",
       "        [1, 0, 2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'x' found in data\n",
      "'edge_index' found in data\n"
     ]
    }
   ],
   "source": [
    "for key, item in data:\n",
    "    print(f\"'{key}' found in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'edge_attr' in data # checks if the data object has an edge_attr attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 3\n",
      "number of edges: 4\n",
      "number of node features: 1\n",
      "number of edge features: 0\n",
      "has isolated nodes: False\n",
      "has self-loops: False\n",
      "is directed: False\n",
      "is undirected: True\n"
     ]
    }
   ],
   "source": [
    "print(\"number of nodes:\", data.num_nodes) # number of nodes in the graph\n",
    "print(\"number of edges:\", data.num_edges) # number of edges in the graph\n",
    "print(\"number of node features:\", data.num_node_features) # number of node features\n",
    "print(\"number of edge features:\", data.num_edge_features) # number of edge features\n",
    "print(\"has isolated nodes:\", data.has_isolated_nodes()) # checks if the graph has isolated nodes\n",
    "print(\"has self-loops:\", data.has_self_loops()) # checks if the graph has self-loops\n",
    "print(\"is directed:\", data.is_directed()) # checks if the graph is directed\n",
    "print(\"is undirected:\", data.is_undirected()) # checks if the graph is undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device) # moves the data object to the device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common benchmark datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUDataset - ENZYMES (Graph Lavel Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
      "Extracting datasets\\ENZYMES\\ENZYMES\\ENZYMES.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='datasets/ENZYMES', name='ENZYMES') # downloads the ENZYMES dataset to /tmp/ENZYMES, a collection of 600 graphs (Data objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "6\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset)) # number of graphs in the dataset\n",
    "print(dataset.num_classes)\n",
    "print(dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENZYMES(600)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 168], x=[37, 3], y=[1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0] # get the first graph object in the dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.is_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.y.shape)\n",
    "dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Lavel Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:540] # first 540 graphs\n",
    "test_dataset = dataset[540:] # last 60 graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENZYMES(540)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Lavel Dataset shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "dataset = dataset.shuffle() # shuffles the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 102], x=[32, 3], y=[1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 174], x=[39, 3], y=[1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method 2\n",
    "perm = torch.randperm(len(dataset)) # generates a random permutation of the indices\n",
    "dataset = dataset[perm] # shuffles the dataset\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planetoid - Cora (Node Lavel Semi-Supervised Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Cora()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(root='datasets/Cora/', name='Cora') # downloads the Cora dataset to here, a collection of 2708 citation graphs (Data objects)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes: 7\n",
      "num node features: 1433\n",
      "num edge features: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 4,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"num classes:\", dataset.num_classes)\n",
    "print(\"num node features:\", dataset.num_node_features)\n",
    "print(\"num edge features:\", dataset.num_edge_features)\n",
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has isolated nodes: False\n",
      "has self-loops: False\n",
      "is directed: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "print(\"has isolated nodes:\", data.has_isolated_nodes())\n",
    "print(\"has self-loops:\", data.has_self_loops())\n",
    "print(\"is directed:\", data.is_directed())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'x' found in data\n",
      "'edge_index' found in data\n",
      "'y' found in data\n",
      "'train_mask' found in data\n",
      "'val_mask' found in data\n",
      "'test_mask' found in data\n"
     ]
    }
   ],
   "source": [
    "for key, item in data:\n",
    "    print(f\"'{key}' found in data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Lavel Dataset splitting (train, val, test) through `masking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  ..., False, False, False])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train_mask'] # a mask to identify the nodes that are to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n",
      "0.051698670605613\n",
      "500\n",
      "0.18463810930576072\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(data.train_mask.sum().item()) # number of nodes to be used for training\n",
    "print(data.train_mask.sum().item() / data.num_nodes) # percentage of nodes to be used for training\n",
    "print(data.val_mask.sum().item()) # number of nodes to be used for validation\n",
    "print(data.val_mask.sum().item() / data.num_nodes) # percentage of nodes to be used for validation\n",
    "print(data.test_mask.sum().item()) # number of nodes to be used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Codes\\learning_pytorch\\learning_pyg\\intro_to_pyg.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y136sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data\u001b[39m.\u001b[39;49mshuffle() \u001b[39m# can't shuffle a data object, i.e., a single graph \u001b[39;00m\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\data.py:482\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m    477\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 482\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_store, key)\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\storage.py:87\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[0;32m     86\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "data.shuffle() # can't shuffle a data object, i.e., a single graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batches and `torch_geometric.data.DataLoader`\n",
    "> PyG achieves parallelization over a mini-batch by creating sparse block diagonal adjacency matrices (defined by `edge_index`) and concatenating feature and target matrices in the node dimension.\n",
    "\n",
    "\\- PyG Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cora = Planetoid(root='datasets/Cora/', name='Cora')\n",
    "dataset_enzymes = TUDataset(root='datasets/ENZYMES/', name='ENZYMES', use_node_attr=True) # use_node_attr=True to use node features(?!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_cora = DataLoader(dataset_cora, batch_size=32, shuffle=True) # creates a data loader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 DataBatch(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], batch=[2708], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "for batch in loader_cora:\n",
    "    print(batch.num_graphs, batch) # a batch of graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENZYMES(600)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_enzymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.loader.dataloader.DataLoader at 0x2814c669490>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_enzymes = DataLoader(dataset_enzymes, batch_size=32, shuffle=True)\n",
    "loader_enzymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32 DataBatch(edge_index=[2, 3288], x=[856, 21], y=[32], batch=[856], ptr=[33])\n",
      "1 32 DataBatch(edge_index=[2, 3780], x=[963, 21], y=[32], batch=[963], ptr=[33])\n",
      "2 32 DataBatch(edge_index=[2, 4472], x=[1267, 21], y=[32], batch=[1267], ptr=[33])\n",
      "3 32 DataBatch(edge_index=[2, 3974], x=[1029, 21], y=[32], batch=[1029], ptr=[33])\n",
      "4 32 DataBatch(edge_index=[2, 3930], x=[965, 21], y=[32], batch=[965], ptr=[33])\n",
      "5 32 DataBatch(edge_index=[2, 4318], x=[1134, 21], y=[32], batch=[1134], ptr=[33])\n",
      "6 32 DataBatch(edge_index=[2, 4050], x=[1021, 21], y=[32], batch=[1021], ptr=[33])\n",
      "7 32 DataBatch(edge_index=[2, 3406], x=[997, 21], y=[32], batch=[997], ptr=[33])\n",
      "8 32 DataBatch(edge_index=[2, 3922], x=[1080, 21], y=[32], batch=[1080], ptr=[33])\n",
      "9 32 DataBatch(edge_index=[2, 3634], x=[979, 21], y=[32], batch=[979], ptr=[33])\n",
      "10 32 DataBatch(edge_index=[2, 4144], x=[1084, 21], y=[32], batch=[1084], ptr=[33])\n",
      "11 32 DataBatch(edge_index=[2, 4308], x=[1109, 21], y=[32], batch=[1109], ptr=[33])\n",
      "12 32 DataBatch(edge_index=[2, 4238], x=[1159, 21], y=[32], batch=[1159], ptr=[33])\n",
      "13 32 DataBatch(edge_index=[2, 4078], x=[1039, 21], y=[32], batch=[1039], ptr=[33])\n",
      "14 32 DataBatch(edge_index=[2, 4026], x=[1020, 21], y=[32], batch=[1020], ptr=[33])\n",
      "15 32 DataBatch(edge_index=[2, 3934], x=[1040, 21], y=[32], batch=[1040], ptr=[33])\n",
      "16 32 DataBatch(edge_index=[2, 4266], x=[1085, 21], y=[32], batch=[1085], ptr=[33])\n",
      "17 32 DataBatch(edge_index=[2, 3820], x=[970, 21], y=[32], batch=[970], ptr=[33])\n",
      "18 24 DataBatch(edge_index=[2, 2976], x=[783, 21], y=[24], batch=[783], ptr=[25])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(loader_enzymes):\n",
    "    print(i, batch.num_graphs, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import ShapeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShapeNet(2349, categories=['Airplane'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_shapenet = ShapeNet(root='datasets/ShapeNet/', categories=['Airplane']) # downloads the Airplane category of the ShapeNet dataset to here, a collection of 2690 graphs (Data objects)\n",
    "dataset_shapenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num data(graphs) 2349\n",
      "num classes: 50\n",
      "num node features: 3\n",
      "num edge features: 0\n",
      "num labels: 6044171\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"num data(graphs)\", len(dataset_shapenet))\n",
    "print(\"num classes:\", dataset_shapenet.num_classes)\n",
    "print(\"num node features:\", dataset_shapenet.num_node_features)\n",
    "print(\"num edge features:\", dataset_shapenet.num_edge_features)\n",
    "print(\"num labels:\", len(dataset_shapenet.y))\n",
    "print(\"\")\n",
    "dataset_shapenet.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2518, 3], y=[2518], pos=[2518, 3], category=[1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_shapenet[0] \n",
    "# 2518 nodes, 3 features per node, 0 edges, 0 edge features, 1 class, 2518 labels (one for each node)\n",
    "# each node has a label, but the graph has a class\n",
    "# each node is a point in 3D space, and the graph is a 3D object (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0392,  0.3344,  0.9416],\n",
       "        [ 0.0011,  0.3488, -0.9372],\n",
       "        [-0.2507,  0.9366,  0.2447],\n",
       "        ...,\n",
       "        [ 0.6270, -0.5863,  0.5130],\n",
       "        [-0.2090,  0.9760, -0.0607],\n",
       "        [-0.2459,  0.9653, -0.0878]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_shapenet[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 3,  ..., 3, 1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_shapenet[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0145, -0.0164,  0.0320],\n",
       "        [-0.0119, -0.0657,  0.0145],\n",
       "        [-0.1424, -0.0370, -0.0519],\n",
       "        ...,\n",
       "        [ 0.0342, -0.0931, -0.0523],\n",
       "        [-0.0108, -0.0600,  0.0522],\n",
       "        [-0.0165, -0.0593,  0.0560]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_shapenet[0].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([1326,  589,  323,  280]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_shapenet[0].y.unique(return_counts=True) # classes of the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying transforms to the dataset\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "'knn_graph' requires 'torch-cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Codes\\learning_pytorch\\learning_pyg\\intro_to_pyg.ipynb Cell 67\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y165sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_shapenet_transformed \u001b[39m=\u001b[39m ShapeNet(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y165sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdatasets/ShapeNet_Transformed/\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y165sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     categories\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mAirplane\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y165sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     pre_transform\u001b[39m=\u001b[39;49mT\u001b[39m.\u001b[39;49mKNNGraph(k\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y165sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ) \u001b[39m# creates a k-NN graph for each graph in the dataset\u001b[39;00m\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\datasets\\shapenet.py:137\u001b[0m, in \u001b[0;36mShapeNet.__init__\u001b[1;34m(self, root, categories, include_normals, split, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(category \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategory_ids \u001b[39mfor\u001b[39;00m category \u001b[39min\u001b[39;00m categories)\n\u001b[0;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m=\u001b[39m categories\n\u001b[1;32m--> 137\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter)\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    140\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_paths[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:76\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     70\u001b[0m     root: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     log: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m ):\n\u001b[1;32m---> 76\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter, log)\n\u001b[0;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:102\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download()\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_process:\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process()\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:235\u001b[0m, in \u001b[0;36mDataset._process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProcessing...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m    234\u001b[0m makedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir)\n\u001b[1;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess()\n\u001b[0;32m    237\u001b[0m path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39m'\u001b[39m\u001b[39mpre_transform.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    238\u001b[0m torch\u001b[39m.\u001b[39msave(_repr(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform), path)\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\datasets\\shapenet.py:216\u001b[0m, in \u001b[0;36mShapeNet.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    212\u001b[0m     filenames \u001b[39m=\u001b[39m [\n\u001b[0;32m    213\u001b[0m         osp\u001b[39m.\u001b[39msep\u001b[39m.\u001b[39mjoin(name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m:]) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m    215\u001b[0m     ]  \u001b[39m# Removing first directory.\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m data_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_filenames(filenames)\n\u001b[0;32m    217\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    218\u001b[0m     trainval \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m data_list\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\datasets\\shapenet.py:201\u001b[0m, in \u001b[0;36mShapeNet.process_filenames\u001b[1;34m(self, filenames)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_transform(data)\n\u001b[0;32m    202\u001b[0m     data_list\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m data_list\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\transforms\\base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     31\u001b[0m     \u001b[39m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(copy\u001b[39m.\u001b[39;49mcopy(data))\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\transforms\\knn_graph.py:52\u001b[0m, in \u001b[0;36mKNNGraph.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     49\u001b[0m data\u001b[39m.\u001b[39medge_attr \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     50\u001b[0m batch \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mbatch \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m data \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m edge_index \u001b[39m=\u001b[39m torch_geometric\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mknn_graph(\n\u001b[0;32m     53\u001b[0m     data\u001b[39m.\u001b[39;49mpos,\n\u001b[0;32m     54\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk,\n\u001b[0;32m     55\u001b[0m     batch,\n\u001b[0;32m     56\u001b[0m     loop\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop,\n\u001b[0;32m     57\u001b[0m     flow\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow,\n\u001b[0;32m     58\u001b[0m     cosine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcosine,\n\u001b[0;32m     59\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_undirected:\n\u001b[0;32m     63\u001b[0m     edge_index \u001b[39m=\u001b[39m to_undirected(edge_index, num_nodes\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mnum_nodes)\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\nn\\pool\\__init__.py:167\u001b[0m, in \u001b[0;36mknn_graph\u001b[1;34m(x, k, batch, loop, flow, cosine, num_workers, batch_size)\u001b[0m\n\u001b[0;32m    164\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch_geometric\u001b[39m.\u001b[39mtyping\u001b[39m.\u001b[39mWITH_TORCH_CLUSTER_BATCH_SIZE:\n\u001b[1;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_cluster\u001b[39m.\u001b[39;49mknn_graph(x, k, batch, loop, flow, cosine,\n\u001b[0;32m    168\u001b[0m                                    num_workers)\n\u001b[0;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m torch_cluster\u001b[39m.\u001b[39mknn_graph(x, k, batch, loop, flow, cosine,\n\u001b[0;32m    170\u001b[0m                                num_workers, batch_size)\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\typing.py:81\u001b[0m, in \u001b[0;36mTorchCluster.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m requires \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch-cluster\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: 'knn_graph' requires 'torch-cluster'"
     ]
    }
   ],
   "source": [
    "dataset_shapenet_transformed = ShapeNet(\n",
    "    root='datasets/ShapeNet_Transformed/', \n",
    "    categories=['Airplane'], \n",
    "    pre_transform=T.KNNGraph(k=6)\n",
    ") # creates a k-NN graph for each graph in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://shapenet.cs.stanford.edu/media/shapenetcore_partanno_segmentation_benchmark_v0_normal.zip\n",
      "Extracting datasets\\ShapeNet_Transformed2\\shapenetcore_partanno_segmentation_benchmark_v0_normal.zip\n",
      "Processing...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "'knn_graph' requires 'torch-cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Codes\\learning_pytorch\\learning_pyg\\intro_to_pyg.ipynb Cell 68\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y166sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset_shapenet_transformed2 \u001b[39m=\u001b[39m ShapeNet(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y166sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdatasets/ShapeNet_Transformed2/\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y166sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     categories\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mAirplane\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y166sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     pre_transform\u001b[39m=\u001b[39;49mT\u001b[39m.\u001b[39;49mKNNGraph(k\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m), \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y166sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     transform\u001b[39m=\u001b[39;49mT\u001b[39m.\u001b[39;49mRandomJitter(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Codes/learning_pytorch/learning_pyg/intro_to_pyg.ipynb#Y166sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ) \u001b[39m# creates a k-NN graph for each graph in the dataset and applies random jittering to the node positions\u001b[39;00m\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\datasets\\shapenet.py:137\u001b[0m, in \u001b[0;36mShapeNet.__init__\u001b[1;34m(self, root, categories, include_normals, split, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(category \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategory_ids \u001b[39mfor\u001b[39;00m category \u001b[39min\u001b[39;00m categories)\n\u001b[0;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategories \u001b[39m=\u001b[39m categories\n\u001b[1;32m--> 137\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter)\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    140\u001b[0m     path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_paths[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:76\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     70\u001b[0m     root: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m     log: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m ):\n\u001b[1;32m---> 76\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter, log)\n\u001b[0;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:102\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download()\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_process:\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process()\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:235\u001b[0m, in \u001b[0;36mDataset._process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProcessing...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m    234\u001b[0m makedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir)\n\u001b[1;32m--> 235\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess()\n\u001b[0;32m    237\u001b[0m path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39m'\u001b[39m\u001b[39mpre_transform.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    238\u001b[0m torch\u001b[39m.\u001b[39msave(_repr(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform), path)\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\datasets\\shapenet.py:216\u001b[0m, in \u001b[0;36mShapeNet.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    212\u001b[0m     filenames \u001b[39m=\u001b[39m [\n\u001b[0;32m    213\u001b[0m         osp\u001b[39m.\u001b[39msep\u001b[39m.\u001b[39mjoin(name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m:]) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    214\u001b[0m         \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m    215\u001b[0m     ]  \u001b[39m# Removing first directory.\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m data_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_filenames(filenames)\n\u001b[0;32m    217\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    218\u001b[0m     trainval \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m data_list\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\datasets\\shapenet.py:201\u001b[0m, in \u001b[0;36mShapeNet.process_filenames\u001b[1;34m(self, filenames)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_transform(data)\n\u001b[0;32m    202\u001b[0m     data_list\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m    204\u001b[0m \u001b[39mreturn\u001b[39;00m data_list\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\transforms\\base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, data: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     31\u001b[0m     \u001b[39m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(copy\u001b[39m.\u001b[39;49mcopy(data))\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\transforms\\knn_graph.py:52\u001b[0m, in \u001b[0;36mKNNGraph.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     49\u001b[0m data\u001b[39m.\u001b[39medge_attr \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     50\u001b[0m batch \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mbatch \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m data \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m edge_index \u001b[39m=\u001b[39m torch_geometric\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mknn_graph(\n\u001b[0;32m     53\u001b[0m     data\u001b[39m.\u001b[39;49mpos,\n\u001b[0;32m     54\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mk,\n\u001b[0;32m     55\u001b[0m     batch,\n\u001b[0;32m     56\u001b[0m     loop\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop,\n\u001b[0;32m     57\u001b[0m     flow\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow,\n\u001b[0;32m     58\u001b[0m     cosine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcosine,\n\u001b[0;32m     59\u001b[0m     num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_workers,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforce_undirected:\n\u001b[0;32m     63\u001b[0m     edge_index \u001b[39m=\u001b[39m to_undirected(edge_index, num_nodes\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mnum_nodes)\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\nn\\pool\\__init__.py:167\u001b[0m, in \u001b[0;36mknn_graph\u001b[1;34m(x, k, batch, loop, flow, cosine, num_workers, batch_size)\u001b[0m\n\u001b[0;32m    164\u001b[0m     batch \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch_geometric\u001b[39m.\u001b[39mtyping\u001b[39m.\u001b[39mWITH_TORCH_CLUSTER_BATCH_SIZE:\n\u001b[1;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_cluster\u001b[39m.\u001b[39;49mknn_graph(x, k, batch, loop, flow, cosine,\n\u001b[0;32m    168\u001b[0m                                    num_workers)\n\u001b[0;32m    169\u001b[0m \u001b[39mreturn\u001b[39;00m torch_cluster\u001b[39m.\u001b[39mknn_graph(x, k, batch, loop, flow, cosine,\n\u001b[0;32m    170\u001b[0m                                num_workers, batch_size)\n",
      "File \u001b[1;32md:\\Installations\\JupyterNotebook\\Lib\\site-packages\\torch_geometric\\typing.py:81\u001b[0m, in \u001b[0;36mTorchCluster.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m requires \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtorch-cluster\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: 'knn_graph' requires 'torch-cluster'"
     ]
    }
   ],
   "source": [
    "dataset_shapenet_transformed2 = ShapeNet(\n",
    "    root='datasets/ShapeNet_Transformed2/', \n",
    "    categories=['Airplane'], \n",
    "    pre_transform=T.KNNGraph(k=6), \n",
    "    transform=T.RandomJitter(0.01)\n",
    ") # creates a k-NN graph for each graph in the dataset and applies random jittering to the node positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Graph Neural Network (GCN) for Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cora()"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(dataset_cora.num_node_features, 16) # 1433 input features, 16 output features\n",
    "        self.conv2 = GCNConv(16, dataset_cora.num_classes) # 16 input features, 7 output features\n",
    "    \n",
    "    def forward(self, data: Data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN().to(device)\n",
    "data = dataset_cora[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) # weight_decay is L2 regularization, i.e., weight decay (dunno what's that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.9252\n",
      "Epoch: 001, Loss: 1.8003\n",
      "Epoch: 002, Loss: 1.6644\n",
      "Epoch: 003, Loss: 1.5170\n",
      "Epoch: 004, Loss: 1.3514\n"
     ]
    }
   ],
   "source": [
    "model.train() # sets the model to training mode\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad() # clears the gradients\n",
    "    z = model(data) # forward pass\n",
    "    loss = F.nll_loss(z[data.train_mask], data.y[data.train_mask]) # loss calculation\n",
    "    loss.backward() # backward pass\n",
    "    optimizer.step() # updates the parameters\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 1.2352\n",
      "Epoch: 006, Loss: 1.0841\n",
      "Epoch: 007, Loss: 0.9245\n",
      "Epoch: 008, Loss: 0.8739\n",
      "Epoch: 009, Loss: 0.7258\n",
      "Epoch: 010, Loss: 0.6591\n",
      "Epoch: 011, Loss: 0.5978\n",
      "Epoch: 012, Loss: 0.4939\n",
      "Epoch: 013, Loss: 0.4380\n",
      "Epoch: 014, Loss: 0.3634\n",
      "Epoch: 015, Loss: 0.3365\n",
      "Epoch: 016, Loss: 0.3247\n",
      "Epoch: 017, Loss: 0.2886\n",
      "Epoch: 018, Loss: 0.2413\n",
      "Epoch: 019, Loss: 0.2352\n",
      "Epoch: 020, Loss: 0.1902\n",
      "Epoch: 021, Loss: 0.1754\n",
      "Epoch: 022, Loss: 0.1424\n",
      "Epoch: 023, Loss: 0.1949\n",
      "Epoch: 024, Loss: 0.1246\n",
      "Epoch: 025, Loss: 0.1290\n",
      "Epoch: 026, Loss: 0.1545\n",
      "Epoch: 027, Loss: 0.0824\n",
      "Epoch: 028, Loss: 0.0985\n",
      "Epoch: 029, Loss: 0.0973\n",
      "Epoch: 030, Loss: 0.0976\n",
      "Epoch: 031, Loss: 0.1153\n",
      "Epoch: 032, Loss: 0.0641\n",
      "Epoch: 033, Loss: 0.0795\n",
      "Epoch: 034, Loss: 0.0830\n",
      "Epoch: 035, Loss: 0.0553\n",
      "Epoch: 036, Loss: 0.0686\n",
      "Epoch: 037, Loss: 0.0540\n",
      "Epoch: 038, Loss: 0.0779\n",
      "Epoch: 039, Loss: 0.0567\n",
      "Epoch: 040, Loss: 0.0547\n",
      "Epoch: 041, Loss: 0.0591\n",
      "Epoch: 042, Loss: 0.0511\n",
      "Epoch: 043, Loss: 0.0393\n",
      "Epoch: 044, Loss: 0.0581\n"
     ]
    }
   ],
   "source": [
    "model.train() # sets the model to training mode\n",
    "for epoch in range(epoch+1, 45):\n",
    "    optimizer.zero_grad() # clears the gradients\n",
    "    z = model(data) # forward pass\n",
    "    loss = F.nll_loss(z[data.train_mask], data.y[data.train_mask]) # loss calculation\n",
    "    loss.backward() # backward pass\n",
    "    optimizer.step() # updates the parameters\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8020\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JupyterNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
