{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing link prediction with PyG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a 'custom' `decoder` function\n",
    "base: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/link_pred.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.data import Data\n",
    "# from torch_geometric.utils import train_test_split_edges\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([ # composite transform\n",
    "    T.NormalizeFeatures(),\n",
    "    T.ToDevice(device),\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1) # 5% for validation, 10% for testing and the rest 85% for training\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cora()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Planetoid(root='datasets/Cora', name='Cora', transform=transform)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_not_transformed = Planetoid(root='datasets/Cora', name='Cora')\n",
    "dataset_not_transformed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[2708, 1433], edge_index=[2, 8974], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[17948], edge_label_index=[2, 17948]),\n",
       " Data(x=[2708, 1433], edge_index=[2, 8974], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[1054], edge_label_index=[2, 1054]),\n",
       " Data(x=[2708, 1433], edge_index=[2, 9501], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[2110], edge_label_index=[2, 2110]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0] # what are the `edge_label` and `edge_label_index`\n",
    "# i think, the `RandomLinkSplit` adds one negative samples for every positive samples in each of the returning graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 8974], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[17948], edge_label_index=[2, 17948])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = dataset[0]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_index == val_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.is_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1.]), tensor([8974, 8974]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_label.unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1867,  201,  598,  ...,  727,  722, 1399],\n",
       "        [2117,  598, 1003,  ...,  608, 1967,  977]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1867,  201,  598,  ..., 2448, 1013, 1359],\n",
       "        [2117,  598, 1003,  ..., 1067, 1661, 2480]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "    \n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "    \n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=1) # sum(subject_nodes * object_nodes)\n",
    "    \n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "    \n",
    "    \n",
    "    def forward(self, data: Data): \n",
    "        z = self.encode(data.x, data.edge_index)\n",
    "        \n",
    "        if not self.training:\n",
    "            return self.decode(z, data.edge_label_index).view(-1)\n",
    "\n",
    "        # calculating a new set of negative samples\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index = train_data.edge_index,\n",
    "            num_nodes = train_data.num_nodes,\n",
    "            num_neg_samples = train_data.edge_label_index.size(1),\n",
    "            method='sparse'\n",
    "        )\n",
    "\n",
    "        # adding those negative sample to the data\n",
    "        edge_label_index = torch.cat([\n",
    "            train_data.edge_label_index, \n",
    "            neg_edge_index\n",
    "        ], dim=-1)\n",
    "\n",
    "        edge_label = torch.cat([\n",
    "            train_data.edge_label,\n",
    "            train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim = 0)\n",
    "\n",
    "        return self.decode(z, edge_label_index).view(-1)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): GCNConv(1433, 128)\n",
       "  (conv2): GCNConv(128, 7)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GCN(dataset.num_features, 128, dataset.num_classes).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 17948])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_label_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # calculating a new round of negative sampling for every every epoch\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index = train_data.edge_index,\n",
    "        num_nodes = train_data.num_nodes,\n",
    "        num_neg_samples = train_data.edge_label_index.size(1),\n",
    "        method='sparse'\n",
    "    )\n",
    "\n",
    "    # adding concatenating the new neg sample with the existing samples\n",
    "    edge_label_index = torch.cat([\n",
    "        train_data.edge_label_index, \n",
    "        neg_edge_index\n",
    "    ], dim=-1)\n",
    "\n",
    "\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim = 0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    \n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the codes bellow this, are direct copy-paste, without reading anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(data: Data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6302, Val: 0.8624, Test: 0.8547\n",
      "Epoch: 002, Loss: 0.6274, Val: 0.8647, Test: 0.8586\n",
      "Epoch: 003, Loss: 0.6283, Val: 0.8661, Test: 0.8607\n",
      "Epoch: 004, Loss: 0.6278, Val: 0.8675, Test: 0.8616\n",
      "Epoch: 005, Loss: 0.6270, Val: 0.8681, Test: 0.8632\n",
      "Epoch: 006, Loss: 0.6254, Val: 0.8688, Test: 0.8651\n",
      "Epoch: 007, Loss: 0.6254, Val: 0.8700, Test: 0.8658\n",
      "Epoch: 008, Loss: 0.6238, Val: 0.8711, Test: 0.8671\n",
      "Epoch: 009, Loss: 0.6267, Val: 0.8712, Test: 0.8680\n",
      "Epoch: 010, Loss: 0.6258, Val: 0.8712, Test: 0.8685\n",
      "Epoch: 011, Loss: 0.6227, Val: 0.8727, Test: 0.8694\n",
      "Epoch: 012, Loss: 0.6230, Val: 0.8729, Test: 0.8691\n",
      "Epoch: 013, Loss: 0.6206, Val: 0.8728, Test: 0.8694\n",
      "Epoch: 014, Loss: 0.6231, Val: 0.8727, Test: 0.8702\n",
      "Epoch: 015, Loss: 0.6238, Val: 0.8731, Test: 0.8703\n",
      "Epoch: 016, Loss: 0.6218, Val: 0.8731, Test: 0.8689\n",
      "Epoch: 017, Loss: 0.6208, Val: 0.8735, Test: 0.8693\n",
      "Epoch: 018, Loss: 0.6223, Val: 0.8738, Test: 0.8701\n",
      "Epoch: 019, Loss: 0.6209, Val: 0.8742, Test: 0.8698\n",
      "Epoch: 020, Loss: 0.6176, Val: 0.8743, Test: 0.8678\n",
      "Epoch: 021, Loss: 0.6188, Val: 0.8752, Test: 0.8699\n",
      "Epoch: 022, Loss: 0.6184, Val: 0.8763, Test: 0.8712\n",
      "Epoch: 023, Loss: 0.6191, Val: 0.8766, Test: 0.8707\n",
      "Epoch: 024, Loss: 0.6190, Val: 0.8771, Test: 0.8711\n",
      "Epoch: 025, Loss: 0.6184, Val: 0.8779, Test: 0.8731\n",
      "Epoch: 026, Loss: 0.6182, Val: 0.8784, Test: 0.8730\n",
      "Epoch: 027, Loss: 0.6189, Val: 0.8788, Test: 0.8726\n",
      "Epoch: 028, Loss: 0.6174, Val: 0.8791, Test: 0.8739\n",
      "Epoch: 029, Loss: 0.6170, Val: 0.8795, Test: 0.8741\n",
      "Epoch: 030, Loss: 0.6148, Val: 0.8801, Test: 0.8751\n",
      "Epoch: 031, Loss: 0.6204, Val: 0.8804, Test: 0.8747\n",
      "Epoch: 032, Loss: 0.6180, Val: 0.8803, Test: 0.8731\n",
      "Epoch: 033, Loss: 0.6182, Val: 0.8810, Test: 0.8747\n",
      "Epoch: 034, Loss: 0.6177, Val: 0.8812, Test: 0.8758\n",
      "Epoch: 035, Loss: 0.6182, Val: 0.8811, Test: 0.8745\n",
      "Epoch: 036, Loss: 0.6144, Val: 0.8808, Test: 0.8730\n",
      "Epoch: 037, Loss: 0.6176, Val: 0.8816, Test: 0.8748\n",
      "Epoch: 038, Loss: 0.6167, Val: 0.8815, Test: 0.8753\n",
      "Epoch: 039, Loss: 0.6178, Val: 0.8810, Test: 0.8736\n",
      "Epoch: 040, Loss: 0.6170, Val: 0.8812, Test: 0.8729\n",
      "Epoch: 041, Loss: 0.6181, Val: 0.8820, Test: 0.8745\n",
      "Epoch: 042, Loss: 0.6156, Val: 0.8817, Test: 0.8749\n",
      "Epoch: 043, Loss: 0.6160, Val: 0.8815, Test: 0.8737\n",
      "Epoch: 044, Loss: 0.6146, Val: 0.8816, Test: 0.8737\n",
      "Epoch: 045, Loss: 0.6152, Val: 0.8822, Test: 0.8749\n",
      "Epoch: 046, Loss: 0.6157, Val: 0.8820, Test: 0.8756\n",
      "Epoch: 047, Loss: 0.6166, Val: 0.8807, Test: 0.8741\n",
      "Epoch: 048, Loss: 0.6162, Val: 0.8812, Test: 0.8753\n",
      "Epoch: 049, Loss: 0.6140, Val: 0.8817, Test: 0.8765\n",
      "Epoch: 050, Loss: 0.6135, Val: 0.8814, Test: 0.8766\n",
      "Epoch: 051, Loss: 0.6138, Val: 0.8800, Test: 0.8760\n",
      "Epoch: 052, Loss: 0.6136, Val: 0.8805, Test: 0.8770\n",
      "Epoch: 053, Loss: 0.6150, Val: 0.8812, Test: 0.8775\n",
      "Epoch: 054, Loss: 0.6150, Val: 0.8808, Test: 0.8768\n",
      "Epoch: 055, Loss: 0.6134, Val: 0.8801, Test: 0.8770\n",
      "Epoch: 056, Loss: 0.6134, Val: 0.8803, Test: 0.8777\n",
      "Epoch: 057, Loss: 0.6155, Val: 0.8813, Test: 0.8789\n",
      "Epoch: 058, Loss: 0.6135, Val: 0.8811, Test: 0.8779\n",
      "Epoch: 059, Loss: 0.6128, Val: 0.8802, Test: 0.8775\n",
      "Epoch: 060, Loss: 0.6126, Val: 0.8806, Test: 0.8789\n",
      "Epoch: 061, Loss: 0.6129, Val: 0.8806, Test: 0.8794\n",
      "Epoch: 062, Loss: 0.6113, Val: 0.8804, Test: 0.8792\n",
      "Epoch: 063, Loss: 0.6128, Val: 0.8802, Test: 0.8790\n",
      "Epoch: 064, Loss: 0.6135, Val: 0.8803, Test: 0.8796\n",
      "Epoch: 065, Loss: 0.6120, Val: 0.8800, Test: 0.8799\n",
      "Epoch: 066, Loss: 0.6124, Val: 0.8795, Test: 0.8794\n",
      "Epoch: 067, Loss: 0.6119, Val: 0.8798, Test: 0.8798\n",
      "Epoch: 068, Loss: 0.6137, Val: 0.8799, Test: 0.8807\n",
      "Epoch: 069, Loss: 0.6120, Val: 0.8792, Test: 0.8803\n",
      "Epoch: 070, Loss: 0.6113, Val: 0.8787, Test: 0.8798\n",
      "Epoch: 071, Loss: 0.6135, Val: 0.8793, Test: 0.8802\n",
      "Epoch: 072, Loss: 0.6128, Val: 0.8797, Test: 0.8809\n",
      "Epoch: 073, Loss: 0.6128, Val: 0.8789, Test: 0.8803\n",
      "Epoch: 074, Loss: 0.6111, Val: 0.8781, Test: 0.8797\n",
      "Epoch: 075, Loss: 0.6111, Val: 0.8793, Test: 0.8809\n",
      "Epoch: 076, Loss: 0.6112, Val: 0.8794, Test: 0.8808\n",
      "Epoch: 077, Loss: 0.6106, Val: 0.8787, Test: 0.8804\n",
      "Epoch: 078, Loss: 0.6132, Val: 0.8785, Test: 0.8805\n",
      "Epoch: 079, Loss: 0.6113, Val: 0.8794, Test: 0.8813\n",
      "Epoch: 080, Loss: 0.6111, Val: 0.8797, Test: 0.8812\n",
      "Epoch: 081, Loss: 0.6119, Val: 0.8792, Test: 0.8804\n",
      "Epoch: 082, Loss: 0.6106, Val: 0.8794, Test: 0.8814\n",
      "Epoch: 083, Loss: 0.6113, Val: 0.8797, Test: 0.8820\n",
      "Epoch: 084, Loss: 0.6115, Val: 0.8799, Test: 0.8817\n",
      "Epoch: 085, Loss: 0.6114, Val: 0.8796, Test: 0.8810\n",
      "Epoch: 086, Loss: 0.6101, Val: 0.8801, Test: 0.8817\n",
      "Epoch: 087, Loss: 0.6094, Val: 0.8803, Test: 0.8822\n",
      "Epoch: 088, Loss: 0.6099, Val: 0.8801, Test: 0.8818\n",
      "Epoch: 089, Loss: 0.6087, Val: 0.8800, Test: 0.8812\n",
      "Epoch: 090, Loss: 0.6106, Val: 0.8805, Test: 0.8820\n",
      "Epoch: 091, Loss: 0.6071, Val: 0.8807, Test: 0.8825\n",
      "Epoch: 092, Loss: 0.6086, Val: 0.8807, Test: 0.8821\n",
      "Epoch: 093, Loss: 0.6108, Val: 0.8799, Test: 0.8809\n",
      "Epoch: 094, Loss: 0.6103, Val: 0.8813, Test: 0.8827\n",
      "Epoch: 095, Loss: 0.6079, Val: 0.8817, Test: 0.8832\n",
      "Epoch: 096, Loss: 0.6113, Val: 0.8809, Test: 0.8821\n",
      "Epoch: 097, Loss: 0.6108, Val: 0.8809, Test: 0.8819\n",
      "Epoch: 098, Loss: 0.6098, Val: 0.8824, Test: 0.8836\n",
      "Epoch: 099, Loss: 0.6081, Val: 0.8826, Test: 0.8838\n",
      "Epoch: 100, Loss: 0.6077, Val: 0.8815, Test: 0.8829\n",
      "Final Test: 0.8838\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = final_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_data)\n",
    "    test_auc = test(test_data)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        final_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Final Test: {final_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JupyterNotebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
